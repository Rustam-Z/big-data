{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Trend Analysis on Twitter live data using Spark Streaming"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import sparkContext & StreamingContext from PySpark library."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a sparkContext with AppName \"SteramingTwitterAnalysis\".<br>\n",
    "Setting the LogLevel of SparkContext to ERROR. This will not print all the logs which are INFO or WARN level.<br>\n",
    "Create Spark Streaming Context using SC (spark context). parameter 10 is the batch interval. <br>\n",
    "Every 10 second the analysis will be done."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sc = SparkContext(appName=\"SteramingTwitterAnalysis\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc = StreamingContext(sc, 10)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Connect to socket broker using ssc (spark streaming context)<br>\n",
    "Host : \"172.31.20.58\" (localhost) & port : 5555"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "socket_stream = ssc.socketTextStream(\"172.31.20.58\", 5555)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "window function parameter sets the Window length. All the analysis will be done on tweets stored for 60 secs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lines = socket_stream.window( 20 )"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Process the Stream:\n",
    "1. Receives tweet message, stored in lines. **Input DStream**\n",
    "2. splits the messages into words. **Apply transformation on DStream : flatMap**\n",
    "3. filters all the words which start with a hashtag(#). **transformation : filter**\n",
    "4. converts the words to lowercase. **transformation : map**\n",
    "5. maps each tag to (word, 1). **transformation : map**\n",
    "6. then reduces and counts occurrences of each hash tag. (action : reduceByKey) hashtags = **output DStream**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hashtags = lines.flatMap( lambda text: text.split( \" \" ) ).filter( lambda word: word.lower().startswith(\"#\") ).map( lambda word: ( word.lower()\n",
    ", 1 ) ).reduceByKey( lambda a,b:a+b)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sort the hashtags based on the counts in decreasing order"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "author_counts_sorted_dstream = hashtags.transform(lambda foo:foo.sortBy(lambda x:x[0].lower()).sortBy(lambda x:x[1],ascending=False))"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print the final analysis: Most popular hashtags on streaming twitter data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "author_counts_sorted_dstream.pprint()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Starting the Spark Streaming:\n",
    "Spark Streaming code we have written till now will not execute, untill we start the ssc.<br>\n",
    "ssc.start() will start the spark streaming context. This is the Action for the whole code. <br>\n",
    "Now it'll create the lineage & DAG & do the lazy evaluation & start running the whole sequesnce of code.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ssc.start()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "awaitTermination() is very important to stop the SSC.<br> \n",
    "When we kill this python process then this signal will be sent to awaitTermination() function.<br> \n",
    "it will finally stop the spark streaming job."
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ssc.awaitTermination()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}